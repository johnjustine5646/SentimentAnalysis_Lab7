{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkkDW8oaO1MBFa+zax1Ys2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johnjustine5646/SentimentAnalysis_Lab7/blob/main/John_522_Lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Find the similarity between two documents"
      ],
      "metadata": {
        "id": "JdRqFRcpU5gd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cLc-4-1UwgR",
        "outputId": "79adb46d-d6a8-49b0-ced8-bbcaa2e921d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between the two documents: 0.4356132262843411\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "with open('/content/text1.txt', 'r', encoding='utf-8') as file:\n",
        "    doc1 = file.read()\n",
        "\n",
        "with open('/content/text2.txt', 'r', encoding='utf-8') as file:\n",
        "    doc2 = file.read()\n",
        "\n",
        "documents = [doc1, doc2]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
        "print(\"Cosine Similarity between the two documents:\", cos_sim[0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(doc1, doc2):\n",
        "    set1 = set(doc1.split())\n",
        "    set2 = set(doc2.split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "jac_sim = jaccard_similarity(doc1, doc2)\n",
        "print(\"Jaccard Similarity between the two documents:\", jac_sim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfN2vno2U5RV",
        "outputId": "51cba02f-b790-4d27-c1f9-f62f2fb9b500"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity between the two documents: 0.09649122807017543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ComrtMzeVGsa",
        "outputId": "bf4cc402-0823-42b8-9b9a-50b4bea3945f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "df = pd.read_csv('/content/Tweets.csv')\n",
        "\n",
        "dataset = df[['text', 'airline_sentiment']]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_freq = defaultdict(lambda: [0, 0])\n",
        "for _, row in dataset.iterrows():\n",
        "    text = row['text']\n",
        "    label = row['airline_sentiment']\n",
        "    words = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]\n",
        "    for word in words:\n",
        "        word_freq[word][label == 'positive'] += 1\n",
        "\n",
        "total_positive = sum(word_freq[word][1] for word in word_freq)\n",
        "total_negative = sum(word_freq[word][0] for word in word_freq)\n",
        "prior_positive = total_positive / (total_positive + total_negative)\n",
        "prior_negative = total_negative / (total_positive + total_negative)\n",
        "\n",
        "def classify(text):\n",
        "    words = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]\n",
        "    log_prob_positive = sum([word_freq[word][1] / total_positive for word in words])\n",
        "    log_prob_negative = sum([word_freq[word][0] / total_negative for word in words])\n",
        "    prob_positive = prior_positive * log_prob_positive\n",
        "    prob_negative = prior_negative * log_prob_negative\n",
        "    return 'positive' if prob_positive > prob_negative else 'negative'\n",
        "\n",
        "test_data = [\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA\"]\n",
        "for text in test_data:\n",
        "    sentiment = classify(text)\n",
        "    print(f\"Sentiment of '{text}': {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaiKBEu5VJ2t",
        "outputId": "4006d746-2d9c-470a-a215-09c7173e7852"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of '@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA': negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ],
      "metadata": {
        "id": "WdYIyor5VNQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        ""
      ],
      "metadata": {
        "id": "UDPJjrhDVLr1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Tweets.csv')"
      ],
      "metadata": {
        "id": "30w9vNtlVQlM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['text'].tolist()\n",
        "labels = df['airline_sentiment'].tolist()\n",
        "\n",
        "texts = df['text'].tolist()\n",
        "labels = df['airline_sentiment'].tolist()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "labels_one_hot = to_categorical(labels_encoded)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "sequences_padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences_padded, labels_one_hot, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=32, input_length=max_len))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(3, activation='softmax'))  # Use 3 neurons in the output layer, with a softmax activation function\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)  # Adjust batch size for efficiency\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMCte6RYVUIh",
        "outputId": "bcaf41aa-b1f1-4902-82a2-8fe072c60bef"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "366/366 [==============================] - 11s 23ms/step - loss: 0.7493 - accuracy: 0.6841\n",
            "Epoch 2/10\n",
            "366/366 [==============================] - 9s 24ms/step - loss: 0.4885 - accuracy: 0.8043\n",
            "Epoch 3/10\n",
            "366/366 [==============================] - 7s 20ms/step - loss: 0.3465 - accuracy: 0.8762\n",
            "Epoch 4/10\n",
            "366/366 [==============================] - 9s 24ms/step - loss: 0.2421 - accuracy: 0.9209\n",
            "Epoch 5/10\n",
            "366/366 [==============================] - 8s 22ms/step - loss: 0.1810 - accuracy: 0.9430\n",
            "Epoch 6/10\n",
            "366/366 [==============================] - 8s 22ms/step - loss: 0.1433 - accuracy: 0.9573\n",
            "Epoch 7/10\n",
            "366/366 [==============================] - 8s 23ms/step - loss: 0.1188 - accuracy: 0.9641\n",
            "Epoch 8/10\n",
            "366/366 [==============================] - 7s 20ms/step - loss: 0.1080 - accuracy: 0.9688\n",
            "Epoch 9/10\n",
            "366/366 [==============================] - 8s 23ms/step - loss: 0.0935 - accuracy: 0.9729\n",
            "Epoch 10/10\n",
            "366/366 [==============================] - 8s 22ms/step - loss: 0.0733 - accuracy: 0.9798\n",
            "92/92 [==============================] - 1s 6ms/step - loss: 0.9895 - accuracy: 0.7804\n",
            "Test Loss: 0.9894591569900513\n",
            "Test Accuracy: 0.7803961634635925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Implement the Sentiment Analysis using LSTM."
      ],
      "metadata": {
        "id": "OwRGodGwVVuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, SpatialDropout1D\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "metadata": {
        "id": "jDpk1PXpVYGG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/Tweets.csv\")"
      ],
      "metadata": {
        "id": "iwpGbGgzVatS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['text']\n",
        "y = pd.get_dummies(data['airline_sentiment']).values\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(X, maxlen=200)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 128, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Accuracy:', accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRxz4s1PVcp5",
        "outputId": "a55d9d1e-05cb-4fc8-e02d-6472f63c1ef3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "183/183 [==============================] - 139s 745ms/step - loss: 0.7267 - accuracy: 0.6937 - val_loss: 0.5526 - val_accuracy: 0.7835\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 139s 761ms/step - loss: 0.4775 - accuracy: 0.8150 - val_loss: 0.4984 - val_accuracy: 0.8009\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 150s 819ms/step - loss: 0.3764 - accuracy: 0.8571 - val_loss: 0.4926 - val_accuracy: 0.8033\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 140s 765ms/step - loss: 0.3245 - accuracy: 0.8784 - val_loss: 0.5051 - val_accuracy: 0.8074\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - 134s 735ms/step - loss: 0.2798 - accuracy: 0.8956 - val_loss: 0.5392 - val_accuracy: 0.8033\n",
            "Epoch 6/10\n",
            "183/183 [==============================] - ETA: 0s - loss: 0.2433 - accuracy: 0.9101Restoring model weights from the end of the best epoch: 3.\n",
            "183/183 [==============================] - 138s 754ms/step - loss: 0.2433 - accuracy: 0.9101 - val_loss: 0.5859 - val_accuracy: 0.7982\n",
            "Epoch 6: early stopping\n",
            "Test Accuracy: 0.8032786846160889\n"
          ]
        }
      ]
    }
  ]
}